import re
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Tuple, List, Optional, Union

from redlines.document import Document

tokenizer = re.compile(r"((?:[^()\s]+|[().?!-])\s*)")
paragraph_pattern = re.compile(r"((?:\n *)+)")
space_pattern = re.compile(r"(\s+)")


def tokenize_text(text: str) -> List[str]:
    return re.findall(tokenizer, text)


def split_paragraphs(text: str) -> List[str]:
    """
    Splits a string into a list of paragraphs. One or more `\n` splits the paragraphs.
    For example, if the text is "Hello\nWorld\nThis is a test", the result will be:
    ['Hello', 'World', 'This is a test']

    :param text: The text to split.
    :return: a list of paragraphs.
    """

    split_text = re.split(paragraph_pattern, text)
    result = []
    for s in split_text:
        if s and not re.fullmatch(space_pattern, s):
            result.append(s.strip())

    return result


def concatenate_paragraphs_and_add_chr_182(text: str) -> str:
    """
    Split paragraphs and concatenate them. Then add a character '¶' between paragraphs.
    For example, if the text is "Hello\nWorld\nThis is a test", the result will be:
    "Hello¶World¶This is a test"

    :param text: The text to split.
    :return: a list of paragraphs.
    """
    paragraphs = split_paragraphs(text)

    result = []
    for p in paragraphs:
        result.append(p)
        result.append(" ¶ ")
        # Add a string ' ¶ ' between paragraphs.
    if len(paragraphs) > 0:
        result.pop()

    return "".join(result)


@dataclass
class Chunk:
    """A chunk of text that is being compared. In some cases, it may be the whole document"""

    text: List[str]
    """The tokens of the chunk"""
    chunk_location: Optional[str]
    """An optional string describing the location of the chunk in the document. For example, a PDF page number"""


@dataclass
class Redline:
    """A redline that is generated by the redlines library"""

    source_chunk: Chunk
    test_chunk: Chunk
    """The chunk of text that is being redlined"""
    opcodes: Tuple[str, int, int, int, int]
    """The opcodes that describe the redline in the chunk. See the difflib documentation for more information"""


class RedlinesProcessor(ABC):
    """
    An abstract class that defines the interface for a redlines processor.
    A redlines processor is a class that takes two documents and generates redlines from them.
    Use this class as a base class if you want to create a custom redlines processor.
    See `WholeDocumentProcessor` for an example of a redlines processor.
    """

    @abstractmethod
    def process(
        self, source: Union[Document, str], test: Union[Document, str]
    ) -> List[Redline]:
        pass


class WholeDocumentProcessor(RedlinesProcessor):
    """
    A redlines processor that compares two documents. It compares the entire documents as a single chunk.
    """

    def __init__(self, character_level_diffing: bool = True):
        self.character_level_diffing = character_level_diffing
        self.source_text = None
        self.test_text = None
        self.source_tokens = None
        self.test_tokens = None
        self._redlines = None

    def process(
        self, source: Union[Document, str], test: Union[Document, str]
    ) -> List[Redline]:
        """
        Compare two documents as a single chunk.
        :param source: The source document to compare.
        :param test: The test document to compare.
        :return: A list of `Redline` that describe the differences between the two documents.
        """
        # Extract text from documents if needed
        self.source_text = source.text if isinstance(source, Document) else source
        self.test_text = test.text if isinstance(test, Document) else test

        # Tokenize the texts
        self.source_tokens = tokenize_text(
            concatenate_paragraphs_and_add_chr_182(self.source_text)
        )
        self.test_tokens = tokenize_text(
            concatenate_paragraphs_and_add_chr_182(self.test_text)
        )

        # Normalize tokens by stripping whitespace for comparison
        # This allows the matcher to focus on content differences rather than whitespace variations
        # while still preserving the original tokens (including whitespace) for display in the output
        seq_source_normalized = [token.strip() for token in self.source_tokens]
        seq_test_normalized = [token.strip() for token in self.test_tokens]

        from difflib import SequenceMatcher

        matcher = SequenceMatcher(None, seq_source_normalized, seq_test_normalized)

        self._redlines = [
            Redline(
                source_chunk=Chunk(text=self.source_tokens, chunk_location=None),
                test_chunk=Chunk(text=self.test_tokens, chunk_location=None),
                opcodes=opcode,
            )
            for opcode in matcher.get_opcodes()
        ]

        if self.character_level_diffing:
            self._redlines = self._refine_single_token_replacements(self._redlines)
        return self._redlines

    def _refine_single_token_replacements(
        self, redlines: List[Redline]
    ) -> List[Redline]:
        refined_redlines = []

        for redline in redlines:
            if redline.opcodes[0] == "replace":
                tag, i1, i2, j1, j2 = redline.opcodes

                # Check if this is a single token replacement
                if i2 - i1 == 1 and j2 - j1 == 1:
                    source_token = redline.source_chunk.text[i1].strip()
                    test_token = redline.test_chunk.text[j1].strip()

                    # Skip tokens containing paragraph markers
                    if "¶" in source_token or "¶" in test_token:
                        refined_redlines.append(redline)
                        continue

                    # Now check for prefix/suffix changes only
                    refined_opcodes = self._character_level_diff_if_prefix_suffix(
                        source_token,
                        test_token,
                        i1,
                        i2,
                        j1,
                        j2,
                        redline.source_chunk,
                        redline.test_chunk,
                    )

                    if refined_opcodes:
                        # Skip refinements that only insert or delete trailing punctuation,
                        # so that tokens like "weekend"->"weekend." are handled as full-token replacements
                        if not (
                            len(refined_opcodes) == 2
                            and refined_opcodes[0].opcodes[0] == "equal"
                            and refined_opcodes[1].opcodes[0] in ("insert", "delete")
                        ):
                            refined_redlines.extend(refined_opcodes)
                            continue

            # If we didn't refine this redline, keep the original
            refined_redlines.append(redline)

        return refined_redlines

    def _character_level_diff_if_prefix_suffix(
        self,
        source_token: str,
        test_token: str,
        i1: int,
        i2: int,
        j1: int,
        j2: int,
        source_chunk: Chunk,
        test_chunk: Chunk,
    ) -> List[Redline]:
        # If tokens are identical, no need for character diffing
        if source_token == test_token:
            return []

        # Find the longest common prefix
        prefix_len = 0
        for i in range(min(len(source_token), len(test_token))):
            if source_token[i] != test_token[i]:
                break
            prefix_len = i + 1

        # Find the longest common suffix
        suffix_len = 0
        for i in range(
            1, min(len(source_token) - prefix_len, len(test_token) - prefix_len) + 1
        ):
            if source_token[-i] != test_token[-i]:
                break
            suffix_len = i

        # Special case for trailing punctuation differences
        # Handle case where one token ends with punctuation and the other doesn't
        if source_token.rstrip(".!?,;:") == test_token.rstrip(".!?,;:") and (
            source_token[-1] in ".!?,;:" or test_token[-1] in ".!?,;:"
        ):
            # Create special handling for this case
            base_token = source_token.rstrip(".!?,;:")
            result = []
            # Add the common part
            if base_token:
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=(
                            "equal",
                            i1,
                            i1 + len(base_token),
                            j1,
                            j1 + len(base_token),
                        ),
                    )
                )
            # Handle the punctuation differences
            if source_token != base_token:
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=(
                            "delete",
                            i1 + len(base_token),
                            i1 + len(source_token),
                            j1 + len(base_token),
                            j1 + len(base_token),
                        ),
                    )
                )
            if test_token != base_token:
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=(
                            "insert",
                            i1 + len(base_token),
                            i1 + len(base_token),
                            j1 + len(base_token),
                            j1 + len(test_token),
                        ),
                    )
                )
            return result

        # If either prefix or suffix is different (but not both), use character-level diffing
        is_prefix_change = prefix_len == 0 and suffix_len > 0
        is_suffix_change = prefix_len > 0 and suffix_len == 0
        is_prefix_and_suffix_same = prefix_len > 0 and suffix_len > 0

        if is_prefix_change or is_suffix_change or is_prefix_and_suffix_same:
            # Create new redlines with character-level opcodes
            result = []

            # For prefix changes
            if prefix_len > 0:
                # Add the common prefix as 'equal'
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=("equal", i1, i1 + prefix_len, j1, j1 + prefix_len),
                    )
                )

            # Add the different middle part
            if len(source_token) - prefix_len - suffix_len > 0:
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=(
                            "delete",
                            i1 + prefix_len,
                            i1 + len(source_token) - suffix_len,
                            j1 + prefix_len,
                            j1 + prefix_len,
                        ),
                    )
                )

            if len(test_token) - prefix_len - suffix_len > 0:
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=(
                            "insert",
                            i1 + prefix_len,
                            i1 + prefix_len,
                            j1 + prefix_len,
                            j1 + len(test_token) - suffix_len,
                        ),
                    )
                )

            # For suffix changes, add the common suffix as 'equal'
            if suffix_len > 0:
                result.append(
                    Redline(
                        source_chunk=source_chunk,
                        test_chunk=test_chunk,
                        opcodes=(
                            "equal",
                            i1 + len(source_token) - suffix_len,
                            i1 + len(source_token),
                            j1 + len(test_token) - suffix_len,
                            j1 + len(test_token),
                        ),
                    )
                )

            return result

        # If not a prefix/suffix change only, return empty list to indicate no refinement
        return []
